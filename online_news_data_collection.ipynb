{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fetch Metadata from GDELT\n",
        "\n",
        "This notebook demonstrates how to query and fetch metadata from the [GDELT Project](https://www.gdeltproject.org/).  \n",
        "It includes simple steps to define keywords, time periods, and locations, and then retrieve structured metadata for further analysis.  \n",
        "\n",
        "## How to use\n",
        "1. Run the setup cells to load the required libraries.  \n",
        "2. Set your search parameters (keywords, dates, and locations).  \n",
        "3. Execute the fetch function to retrieve metadata from GDELT.  \n",
        "4. Save or analyze the results directly within the notebook.  \n",
        "\n",
        "> Tip: You can also run this notebook on [Google Colab](https://colab.research.google.com/) without any local setup.  \n"
      ],
      "metadata": {
        "id": "8aR5euCf6ZqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "#  SETUP: Google Drive & Installations\n",
        "# ========================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "yIcmRxQB6eOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdeltdoc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tleTHRN2dLYT",
        "outputId": "2cc2e438-9260-48a0-81fd-7f35f812b1d4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdeltdoc in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: pandas>=2.2.3 in /usr/local/lib/python3.12/dist-packages (from gdeltdoc) (2.3.2)\n",
            "Requirement already satisfied: requests>=2.32.3 in /usr/local/lib/python3.12/dist-packages (from gdeltdoc) (2.32.4)\n",
            "Requirement already satisfied: typing-extensions>=4.13.0 in /usr/local/lib/python3.12/dist-packages (from gdeltdoc) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->gdeltdoc) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->gdeltdoc) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->gdeltdoc) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->gdeltdoc) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.3->gdeltdoc) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.3->gdeltdoc) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.3->gdeltdoc) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.3->gdeltdoc) (2025.8.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.3->gdeltdoc) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "gdelt_fetch_exclusive_end.py\n",
        "----------------------------\n",
        "A script to fetch news articles from the GDELT Doc API based on:\n",
        "- A keyword (e.g., \"Climate Change\")\n",
        "- A specific country filter (e.g., \"UK\")\n",
        "- A date range (start_date -> end_date, inclusive)\n",
        "Uses an exclusive daily fetch approach (end_date = start_date + 1 day)\n",
        "\n",
        "Features:\n",
        " - Logging + print statements for day-by-day fetch info\n",
        " - Rate limiting (10s between each request, +50s after every 100 requests)\n",
        " - Retry logic for transient API failures\n",
        " - Monthly partial saves & final save (all with \"gdelt\" prefix)\n",
        " - Graceful handling of 0-article days\n",
        "\"\"\"\n",
        "\n",
        "import logging\n",
        "import time\n",
        "import shutil\n",
        "from datetime import date, timedelta\n",
        "import pandas as pd\n",
        "from gdeltdoc import GdeltDoc, Filters\n",
        "import re\n",
        "\n",
        "# -------------------------------------------------------------------------------\n",
        "# USER-CONFIGURABLE PARAMETERS\n",
        "# -------------------------------------------------------------------------------\n",
        "KEYWORD = \"volleyball\"\n",
        "COUNTRY = \"Italy\"\n",
        "START_DATE = date(2025, 9, 1)   # inclusive start date\n",
        "END_DATE = date(2025, 9, 10)    # inclusive end date\n",
        "\n",
        "MAX_RECORDS_PER_DAY = 250\n",
        "GDRIVE_DEST = \"/content/drive/MyDrive/sport/\"\n",
        "\n",
        "# Rate-limit constants\n",
        "REQUEST_PAUSE_SECONDS = 1\n",
        "LONG_PAUSE_AFTER_REQUESTS = 1\n",
        "LONG_PAUSE_DURATION = 1\n",
        "\n",
        "# Retry logic\n",
        "MAX_RETRIES = 2\n",
        "RETRY_SLEEP_SECONDS = 2\n",
        "\n",
        "# -------------------------------------------------------------------------------\n",
        "def generate_date_list(start_d: date, end_d: date):\n",
        "    \"\"\"\n",
        "    Generates a list of datetime.date objects for each day in the inclusive range.\n",
        "    \"\"\"\n",
        "    date_list = []\n",
        "    current = start_d\n",
        "    while current <= end_d:\n",
        "        date_list.append(current)\n",
        "        current += timedelta(days=1)\n",
        "    return date_list\n",
        "\n",
        "def clean_filename_part(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Make a string safe for filenames (no spaces or special chars).\n",
        "    \"\"\"\n",
        "    text = text.strip()\n",
        "    text = text.replace(\" \", \"_\")\n",
        "    text = re.sub(r\"[^\\w\\-_]+\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def fetch_articles_for_date(\n",
        "    gd: GdeltDoc,\n",
        "    keyword: str,\n",
        "    country: str,\n",
        "    start_str: str,\n",
        "    end_str: str,\n",
        "    max_records: int\n",
        ") -> pd.DataFrame or None:\n",
        "    \"\"\"\n",
        "    Fetch articles from GDELT for [start_str, end_str),\n",
        "    retrying up to MAX_RETRIES times on transient errors.\n",
        "    \"\"\"\n",
        "    filters = Filters(\n",
        "        keyword=keyword,\n",
        "        start_date=start_str,\n",
        "        end_date=end_str,\n",
        "        num_records=max_records,\n",
        "        country=country\n",
        "    )\n",
        "\n",
        "    for attempt in range(1, MAX_RETRIES + 1):\n",
        "        try:\n",
        "            articles_df = gd.article_search(filters)\n",
        "            return articles_df\n",
        "        except Exception as e:\n",
        "            logging.error(\n",
        "                f\"[fetch_articles_for_date] Attempt {attempt}/{MAX_RETRIES} failed. \"\n",
        "                f\"Date range: {start_str} -> {end_str}, keyword='{keyword}', country='{country}'\\n\"\n",
        "                f\"Error: {e}\"\n",
        "            )\n",
        "            if attempt < MAX_RETRIES:\n",
        "                logging.info(f\"Retrying after {RETRY_SLEEP_SECONDS}s...\")\n",
        "                print(f\"Retrying after {RETRY_SLEEP_SECONDS}s...\")\n",
        "                time.sleep(RETRY_SLEEP_SECONDS)\n",
        "            else:\n",
        "                logging.error(\n",
        "                    f\"All {MAX_RETRIES} attempts failed for {start_str} -> {end_str}. Skipping.\"\n",
        "                )\n",
        "                print(f\"All {MAX_RETRIES} attempts failed for {start_str} -> {end_str}. Skipping.\")\n",
        "                return None\n",
        "    return None\n",
        "\n",
        "def save_to_drive(local_filename: str, drive_folder: str):\n",
        "    \"\"\"\n",
        "    Copy file to Google Drive folder using shutil.copy.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        shutil.copy(local_filename, drive_folder)\n",
        "        logging.info(f\"File '{local_filename}' copied to '{drive_folder}'.\")\n",
        "        print(f\"File '{local_filename}' copied to Google Drive folder: {drive_folder}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error copying file to Google Drive: {e}\")\n",
        "        print(f\"Error copying file to Google Drive: {e}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function orchestrates:\n",
        "     - Generate daily date list\n",
        "     - Fetch data day-by-day, with logging & print statements\n",
        "     - Rate-limit (10s between calls, +50s every 100 requests)\n",
        "     - Save partial monthly CSV + final CSV to local, and copy to GDrive\n",
        "    \"\"\"\n",
        "\n",
        "    # Set up logging\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
        "        datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
        "    )\n",
        "\n",
        "    # Build short \"safe\" versions for filenames\n",
        "    safe_keyword = clean_filename_part(KEYWORD)\n",
        "    safe_country = clean_filename_part(COUNTRY)\n",
        "    date_range_label = f\"{START_DATE.strftime('%Y%m%d')}_to_{END_DATE.strftime('%Y%m%d')}\"\n",
        "\n",
        "    # Final CSV name, including \"gdelt\" prefix\n",
        "    final_csv_filename = f\"gdelt_{safe_keyword}_{date_range_label}_{safe_country}.csv\"\n",
        "\n",
        "    # Logging + print\n",
        "    logging.info(\"Starting GDELT Fetch Script...\")\n",
        "    print(\"Starting GDELT Fetch Script...\")\n",
        "\n",
        "    gd = GdeltDoc()\n",
        "    dates_to_fetch = generate_date_list(START_DATE, END_DATE)\n",
        "    total_days = len(dates_to_fetch)\n",
        "\n",
        "    logging.info(\n",
        "        f\"Fetching data from {START_DATE} to {END_DATE} (inclusive), \"\n",
        "        f\"keyword='{KEYWORD}', country='{COUNTRY}'. Total days: {total_days}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"Fetching data from {START_DATE} to {END_DATE} (inclusive), \"\n",
        "        f\"keyword='{KEYWORD}', country='{COUNTRY}'. Total days: {total_days}\"\n",
        "    )\n",
        "\n",
        "    all_daily_data = []\n",
        "    monthly_data = []\n",
        "    request_count = 0\n",
        "    current_month = START_DATE.month\n",
        "\n",
        "    # Loop over dates\n",
        "    for idx, single_date in enumerate(dates_to_fetch, start=1):\n",
        "        start_str = single_date.strftime(\"%Y-%m-%d\")\n",
        "        next_day = single_date + timedelta(days=1)\n",
        "        end_str = next_day.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "        logging.info(f\"[Day {idx}/{total_days}] Querying {start_str} -> {end_str} (exclusive).\")\n",
        "        print(f\"[Day {idx}/{total_days}] Querying {start_str} -> {end_str} (exclusive).\")\n",
        "\n",
        "        # Fetch\n",
        "        day_df = fetch_articles_for_date(\n",
        "            gd=gd,\n",
        "            keyword=KEYWORD,\n",
        "            country=COUNTRY,\n",
        "            start_str=start_str,\n",
        "            end_str=end_str,\n",
        "            max_records=MAX_RECORDS_PER_DAY\n",
        "        )\n",
        "        request_count += 1\n",
        "\n",
        "        # Evaluate result\n",
        "        if day_df is None or day_df.empty:\n",
        "            msg = f\"No data returned for {start_str}. (Day {idx} of {total_days})\"\n",
        "            logging.warning(msg)\n",
        "            print(msg)\n",
        "        else:\n",
        "            count_msg = f\"Day {start_str}: fetched {len(day_df)} articles.\"\n",
        "            logging.info(count_msg)\n",
        "            print(count_msg)\n",
        "            all_daily_data.append(day_df)\n",
        "            monthly_data.append(day_df)\n",
        "\n",
        "        # Rate-limit pause\n",
        "        logging.info(f\"Pausing {REQUEST_PAUSE_SECONDS}s to respect rate limits...\")\n",
        "        print(f\"Pausing {REQUEST_PAUSE_SECONDS}s...\")\n",
        "        time.sleep(REQUEST_PAUSE_SECONDS)\n",
        "\n",
        "        if request_count % LONG_PAUSE_AFTER_REQUESTS == 0:\n",
        "            extra_pause_msg = (\n",
        "                f\"Reached {request_count} requests. Extra pause {LONG_PAUSE_DURATION}s...\"\n",
        "            )\n",
        "            logging.info(extra_pause_msg)\n",
        "            print(extra_pause_msg)\n",
        "            time.sleep(LONG_PAUSE_DURATION)\n",
        "\n",
        "        # Check if we crossed a month boundary or are at the end\n",
        "        if (next_day.month != current_month) or (single_date == END_DATE):\n",
        "            if monthly_data:\n",
        "                # Combine monthly portion\n",
        "                month_df = pd.concat(monthly_data, ignore_index=True)\n",
        "                before_dedup = len(month_df)\n",
        "                month_df.drop_duplicates(subset=[\"url\", \"seendate\"], inplace=True)\n",
        "                after_dedup = len(month_df)\n",
        "\n",
        "                dedup_msg = (\n",
        "                    f\"Monthly dedup for month={current_month}: \"\n",
        "                    f\"removed {before_dedup - after_dedup}, final count={after_dedup}.\"\n",
        "                )\n",
        "                logging.info(dedup_msg)\n",
        "                print(dedup_msg)\n",
        "\n",
        "                partial_filename = (\n",
        "                    f\"gdelt_{safe_keyword}_\"\n",
        "                    f\"{single_date.strftime('%Y')}_\"\n",
        "                    f\"{str(current_month).zfill(2)}_\"\n",
        "                    f\"{safe_country}.csv\"\n",
        "                )\n",
        "                month_df.to_csv(partial_filename, index=False)\n",
        "                logging.info(f\"Saved monthly file '{partial_filename}'.\")\n",
        "                print(f\"Saved monthly file '{partial_filename}'.\")\n",
        "\n",
        "                save_to_drive(partial_filename, GDRIVE_DEST)\n",
        "\n",
        "                monthly_data = []\n",
        "\n",
        "            current_month = next_day.month\n",
        "\n",
        "    if not all_daily_data:\n",
        "        warning_msg = \"No articles fetched at all for the specified range.\"\n",
        "        logging.warning(warning_msg)\n",
        "        print(warning_msg)\n",
        "        return\n",
        "\n",
        "    # Combine final\n",
        "    combined_df = pd.concat(all_daily_data, ignore_index=True)\n",
        "    pre_final = len(combined_df)\n",
        "    combined_df.drop_duplicates(subset=[\"url\", \"seendate\"], inplace=True)\n",
        "    post_final = len(combined_df)\n",
        "    final_dupes_msg = (\n",
        "        f\"Final dedup removed {pre_final - post_final}. \"\n",
        "        f\"Final total: {post_final} articles.\"\n",
        "    )\n",
        "    logging.info(final_dupes_msg)\n",
        "    print(final_dupes_msg)\n",
        "\n",
        "    # Save final CSV\n",
        "    combined_df.to_csv(final_csv_filename, index=False)\n",
        "    logging.info(f\"Saved final CSV to '{final_csv_filename}'.\")\n",
        "    print(f\"Saved final CSV to '{final_csv_filename}'.\")\n",
        "\n",
        "    save_to_drive(final_csv_filename, GDRIVE_DEST)\n",
        "\n",
        "    done_msg = \"All done!\"\n",
        "    logging.info(done_msg)\n",
        "    print(done_msg)\n",
        "\n",
        "# -------------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYk2TkTw8OWx",
        "outputId": "ec282704-2802-4b1c-87f5-375a309221cb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting GDELT Fetch Script...\n",
            "Fetching data from 2025-09-01 to 2025-09-10 (inclusive), keyword='volleyball', country='Italy'. Total days: 10\n",
            "[Day 1/10] Querying 2025-09-01 -> 2025-09-02 (exclusive).\n",
            "Day 2025-09-01: fetched 31 articles.\n",
            "Pausing 1s...\n",
            "Reached 1 requests. Extra pause 1s...\n",
            "[Day 2/10] Querying 2025-09-02 -> 2025-09-03 (exclusive).\n",
            "Day 2025-09-02: fetched 27 articles.\n",
            "Pausing 1s...\n",
            "Reached 2 requests. Extra pause 1s...\n",
            "[Day 3/10] Querying 2025-09-03 -> 2025-09-04 (exclusive).\n",
            "Day 2025-09-03: fetched 36 articles.\n",
            "Pausing 1s...\n",
            "Reached 3 requests. Extra pause 1s...\n",
            "[Day 4/10] Querying 2025-09-04 -> 2025-09-05 (exclusive).\n",
            "Day 2025-09-04: fetched 49 articles.\n",
            "Pausing 1s...\n",
            "Reached 4 requests. Extra pause 1s...\n",
            "[Day 5/10] Querying 2025-09-05 -> 2025-09-06 (exclusive).\n",
            "Day 2025-09-05: fetched 38 articles.\n",
            "Pausing 1s...\n",
            "Reached 5 requests. Extra pause 1s...\n",
            "[Day 6/10] Querying 2025-09-06 -> 2025-09-07 (exclusive).\n",
            "Day 2025-09-06: fetched 94 articles.\n",
            "Pausing 1s...\n",
            "Reached 6 requests. Extra pause 1s...\n",
            "[Day 7/10] Querying 2025-09-07 -> 2025-09-08 (exclusive).\n",
            "Day 2025-09-07: fetched 169 articles.\n",
            "Pausing 1s...\n",
            "Reached 7 requests. Extra pause 1s...\n",
            "[Day 8/10] Querying 2025-09-08 -> 2025-09-09 (exclusive).\n",
            "Day 2025-09-08: fetched 93 articles.\n",
            "Pausing 1s...\n",
            "Reached 8 requests. Extra pause 1s...\n",
            "[Day 9/10] Querying 2025-09-09 -> 2025-09-10 (exclusive).\n",
            "Day 2025-09-09: fetched 54 articles.\n",
            "Pausing 1s...\n",
            "Reached 9 requests. Extra pause 1s...\n",
            "[Day 10/10] Querying 2025-09-10 -> 2025-09-11 (exclusive).\n",
            "Day 2025-09-10: fetched 36 articles.\n",
            "Pausing 1s...\n",
            "Reached 10 requests. Extra pause 1s...\n",
            "Monthly dedup for month=9: removed 2, final count=625.\n",
            "Saved monthly file 'gdelt_volleyball_2025_09_Italy.csv'.\n",
            "File 'gdelt_volleyball_2025_09_Italy.csv' copied to Google Drive folder: /content/drive/MyDrive/sport/\n",
            "Final dedup removed 2. Final total: 625 articles.\n",
            "Saved final CSV to 'gdelt_volleyball_20250901_to_20250910_Italy.csv'.\n",
            "File 'gdelt_volleyball_20250901_to_20250910_Italy.csv' copied to Google Drive folder: /content/drive/MyDrive/sport/\n",
            "All done!\n"
          ]
        }
      ]
    }
  ]
}